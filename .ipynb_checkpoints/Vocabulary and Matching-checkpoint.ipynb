{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Rule-based matching\n",
    "\n",
    "SpaCy’s rule-based matcher engines and components not only let you find the words and phrases you’re looking for ,they also give you access to the tokens within the document and their relationships\n",
    "\n",
    "This means you can easily access and analyze the surrounding tokens, merge spans into single tokens or add entries to the named entities in doc.ents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Matcher library\n",
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab) # create matcher object and pass nlp.vocab\n",
    "\n",
    "# Here matcher is an object that pairs to current Vocab object\n",
    "# We can add and remove specific named matchers to matcher as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list, and inside that list add series of dictionaries\n",
    "\n",
    "# Hello World can appear in the following ways,\n",
    "# 1) Hello World hello world HellO WORLd\n",
    "# 2) Hello-World\n",
    "\n",
    "pattern_1 = [{'LOWER': 'hello'}, {'LOWER': 'world'}] \n",
    "pattern_2 = [{'LOWER': 'hello'}, {'IS_PUNCT': True}, {'LOWER': 'world'}]  # there should be some punctuation between the words\n",
    "    \n",
    "# 'LOWER', 'IS_PUNCT' are the attributes\n",
    "# they has to be written in  that way only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add patterns to matcher object\n",
    "\n",
    "# Add a match rule to matcher, A match rule consists of,\n",
    "# 1) An ID key\n",
    "# 2) an on_match callback\n",
    "# 3) one or more patterns\n",
    "\n",
    "matcher.add('find_hello_world', None, pattern_1, pattern_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a document\n",
    "doc = nlp(\" 'Hello World' are the first two printed HellO WORLD words for hello, world most of the programmers, printing 'Hello-World' is most common for beginners\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 'Hello World' are the first two printed HellO WORLD words for hello, world most of the programmers, printing 'Hello-World' is most common for beginners"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(8752807494209775780, 2, 4), (8752807494209775780, 10, 12), (8752807494209775780, 14, 17), (8752807494209775780, 24, 27)]\n"
     ]
    }
   ],
   "source": [
    "find_matches = matcher(doc) # passing doc to matcher object and store this in a variable \n",
    "print(find_matches)\n",
    "\n",
    "# it returns output list of tuples\n",
    "# string ID, index start and index end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8752807494209775780 find_hello_world 2 4 Hello World\n",
      "8752807494209775780 find_hello_world 10 12 HellO WORLD\n",
      "8752807494209775780 find_hello_world 14 17 hello, world\n",
      "8752807494209775780 find_hello_world 24 27 Hello-World\n"
     ]
    }
   ],
   "source": [
    "# define a function to find the matches\n",
    "\n",
    "for match_id, start, end in find_matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # get string representation\n",
    "    span = doc[start:end]                    # get the matched span\n",
    "    print(match_id, string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting pattern options and qalifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine the patterns:\n",
    "pattern_3 = [{'LOWER': 'hello'}, {'LOWER': 'world'}]\n",
    "pattern_4 = [{'LOWER': 'hello'}, {'IS_PUNCT': True, 'OP':'*'}, {'LOWER': 'world'}]\n",
    "# 'OP':'*' ----> Thisis going to allow this pattern to match zero or more times for any punctuation\n",
    "\n",
    "# Add the new set of patterns to the 'Hellow World' matcher:\n",
    "matcher.add('Hello World', None, pattern_3, pattern_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_2 = nlp(\"You can print Hello World or hello,& world or Hello-World\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the matches\n",
    "matcher.remove('find_hello_world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(8585552006568828647, 3, 5), (8585552006568828647, 6, 10), (8585552006568828647, 11, 14)]\n"
     ]
    }
   ],
   "source": [
    "find_matches = matcher(doc_2)\n",
    "print(find_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8585552006568828647 Hello World 3 5 Hello World\n",
      "8585552006568828647 Hello World 6 10 hello,& world\n",
      "8585552006568828647 Hello World 11 14 Hello-World\n"
     ]
    }
   ],
   "source": [
    "for match_id, start, end in find_matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # get string representation\n",
    "    span = doc_2[start:end]                    # get the matched span\n",
    "    print(match_id, string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Phrase matching\n",
    "\n",
    "In the above section we used token patterns to perform rule-based matching. An alternative and more efficient method is to match on terminology lists\n",
    "\n",
    "In this case we use PhraseMatcher to create a Doc object from a list of phrases, and pass that into matcher instead\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the PhraseMatcher library\n",
    "from spacy.matcher import PhraseMatcher\n",
    "matcher = PhraseMatcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_list = [\"Barack Obama\", \"Angela Merkel\", \"Washington, D.C.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each phrase to a document object\n",
    "phrase_patterns = [nlp(text) for text in phrase_list] # to do that we are using list comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Barack Obama, Angela Merkel, Washington, D.C.]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase_patterns\n",
    "# phrase objects are not strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(phrase_patterns[0])\n",
    "# they are the spacy docs\n",
    "# thats why we don't have any quotes there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass each doc object into the matcher\n",
    "matcher.add(\"TerminologyList\", None, *phrase_patterns)\n",
    "# we have to add asterisk mark before phrase_pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_3 = nlp(\"German Chancellor Angela Merkel and US President Barack Obama \"\n",
    "          \"converse in the Oval Office inside the White House in Washington, D.C.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3766102292120407359, 2, 4), (3766102292120407359, 7, 9), (3766102292120407359, 19, 22)]\n"
     ]
    }
   ],
   "source": [
    "find_matches = matcher(doc_3) # passin doc to matcher object and store this in a variable \n",
    "print(find_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3766102292120407359 TerminologyList 2 4 Angela Merkel\n",
      "3766102292120407359 TerminologyList 7 9 Barack Obama\n",
      "3766102292120407359 TerminologyList 19 22 Washington, D.C.\n"
     ]
    }
   ],
   "source": [
    "# define a function to find the matches\n",
    "\n",
    "for match_id, start, end in find_matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # get string representation\n",
    "    span = doc_3[start:end]                    # get the matched span\n",
    "    print(match_id, string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
